# Deep Learning Foundations

A list of the papers introducing the most popular advancement in deep learning. A list that is curated to only present the essentials, the hit singles of deep learning as we know it if you will. I decided not to include any advanced papers as I consider that they are of little use for beginners and practionners with no interest in advanced research.
These are the papers that I consider paramount to read and understand as they present the challenges they tried to tackle and as a result gives some insight into neural networks practical uses. 


## Architectures

** Feed forward networks **

** Convolutional neural networks **


## Activation Functions

** Sigmoid **

** tanh **

** Relu **

** LeakyRelu **

** alpha-Relu **

**Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)**, Djork-Arn√© Clevert, Thomas Unterthiner, Sepp Hochreiter (2016) [[pdf]](https://arxiv.org/pdf/1511.07289).


## Gradient Descent 

** Stochastic Gradient Descent **

**Adam: A Method for Stochastic Optimization**, Diederik P. Kingma, Jimmy Ba (2014) [[pdf]](http://arxiv.org/abs/1412.6980)

** Adagrad **

** Adadelta **

** RMSProp **

** FTRL **


## Regularization

**Dropout: A Simple Way to Prevent Neural Networks from Overfitting**, Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky
Ilya Sutskever, Ruslan Salakhutdinov (2014) [[pdf]](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf)

**Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift**, Sergey Ioffe, Christian Szegedy (2015) [[pdf]](https://arxiv.org/pdf/1502.03167)

** Gradient Clipping **


## Hyperparamter tuning

**  **

**  **

**  **
