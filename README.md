# Deep Learning Foundations

A list of the papers introducing the most popular advancement in deep learning. A list that is curated to only present the essentials, the hit singles of deep learning as we know it if you will. I decided not to include any advanced papers as I consider that they are of little use for beginners and practionners with no interest in advanced research.
These are the papers that I consider paramount to read and understand as they present the challenges they tried to tackle and as a result gives some insight into neural networks practical uses. 


## Architectures

** Feed forward networks **

** Convolutional neural networks **


## Activation Functions

** Sigmoid **

** tanh **

** Relu **

** LeakyRelu **

** alpha-Relu **

#### ELU
**Fast and Accurate Deep Network Learning by Exponential Linear Units**, Djork-Arn√© Clevert, Thomas Unterthiner, Sepp Hochreiter (2016) [[pdf]](https://arxiv.org/pdf/1511.07289).


## Gradient Descent 

** Stochastic Gradient Descent **

** Adam **

** Adagrad **

** Adadelta **

** RMSProp **

** FTRL **


## Regularization

#### Dropout
**Dropout: A Simple Way to Prevent Neural Networks from Overfitting**, Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky
Ilya Sutskever, Ruslan Salakhutdinov (2014) [[pdf]](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf)

**Batch Normalization**

** Gradient Clipping **


## Hyperparamter tuning

**  **

**  **

**  **
