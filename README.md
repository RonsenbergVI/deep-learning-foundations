# Deep Learning Foundations

A list of the papers introducing the most popular advancement in deep learning. A list that is curated to only present the essentials, the hit singles of deep learning as we know it if you will. I decided not to include any advanced papers as I consider that they are of little use for beginners and practionners with no interest in advanced research.
These are the papers that I consider paramount to read and understand as they present the challenges they tried to tackle and as a result gives some insight into neural networks practical uses.


## Architectures

** Feed forward networks **

** Convolutional neural networks **


## Activation Functions

#### Relu
**Deep sparse rectifier neural networks**, Glorot Xavier, Antoine Bordes, and Yoshua Bengio (2011) [[pdf]](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf)

#### LeakyRelu
**Rectifier Nonlinearities Improve Neural Network Acoustic Models**  Andrew L. Maas Awni Y. Hannun Andrew Y. Ng (2012) [[pdf]](https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf)

#### PRelu
**Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification** Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (2015) [[pdf]](https://arxiv.org/pdf/1502.01852.pdf)

#### ELU
**Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)**, Djork-Arné Clevert, Thomas Unterthiner, Sepp Hochreiter (2016) [[pdf]](https://arxiv.org/pdf/1511.07289).


## Gradient Descent

#### Best practices
**Efficient Backprop** LeCun, Yann A., Léon Bottou, Genevieve B. Orr, and Klaus-Robert Müller (2012) [[pdf]](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)

#### Adam
**Adam: A Method for Stochastic Optimization**, Diederik P. Kingma, Jimmy Ba (2014) [[pdf]](http://arxiv.org/abs/1412.6980)

#### Follow the regularized leader
**Ad Click Prediction: a View from the Trenches**, H. Brendan McMahan, Gary Holt, D. Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, Sharat Chikkerur, Dan Liu, Martin Wattenberg, Arnar Mar Hrafnkelsson, Tom Boulos, Jeremy Kubica (2013) [[pdf]](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf)

## Regularization

**Dropout: A Simple Way to Prevent Neural Networks from Overfitting**, Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky
Ilya Sutskever, Ruslan Salakhutdinov (2014) [[pdf]](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf)

**Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift**, Sergey Ioffe, Christian Szegedy (2015) [[pdf]](https://arxiv.org/pdf/1502.03167)

**Overtraining, regularization, and searching for minimum in neural networks**, Sjöberg Jonas, Lennart Ljung (1992) [[pdf]](https://pdfs.semanticscholar.org/fa17/eeed3fb2c13cc8c4048d828ff6e38b25d6f3.pdf)
